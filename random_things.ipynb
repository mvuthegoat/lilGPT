{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc419f2d-b4ce-4d33-98b9-b9b0a6aa3bdf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhvn/miniconda3/envs/llama-adapter/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['url', 'article'],\n",
       "        num_rows: 6616\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = load_dataset(\"wiki_lingua\", \"vietnamese\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b472b68f-77f3-471e-a426-5b18ab754ce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6616"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9058149-75ea-4eaf-96df-f963837388a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hoa quả và rau thường rẻ hơn khi vào mùa. Thường thì các cửa hàng thực phẩm sẽ giảm giá cho các mặt hàng nông sản đang vào mùa vì họ biết khách hàng luôn chờ đợi những loại nào vào thời gian nào trong năm. Hơn nữa rau quả đúng mùa bao giờ cũng ngon hơn.  Vào mùa thu, bạn nên chọn các sản phẩm như bí ngô, bí mùa đông, cải bắp và táo. Mùa đông đến, bạn hãy tìm bí mùa đông, cải bắp và các loại củ khác. Mùa xuân là mùa của các loại rau củ như củ cải, rau lá xanh, hành lá và măng tây. Mùa hè là dịp tốt để thưởng thức nhiều loại rau quả, đơn cử như dưa hấu, ngô và các loại quả mọng. Có lẽ bạn cũng nhận thấy là giá cả của rau củ quả thường rẻ nhất trong mùa hè. Ngoài ra, vì nông sản mùa hè rẻ hơn nên bạn có thể mua nhiều để đông lạnh hoặc tự đóng hộp. Có lẽ bạn thích hoa quả và rau củ tươi hơn, nhưng thường thì bạn có thể tiết kiệm hơn nếu chọn sản phẩm đông lạnh hoặc đóng hộp. Những sản phẩm này cũng có lợi cho sức khỏe tương đương như sản phẩm tươi, nhưng bạn cần chọn các loại không thêm đường hoặc muối. Bạn cũng nên xem xét cả nhóm thực phẩm protein. Gà đông lạnh có thể rẻ hơn gà tươi; cá hồi và cá ngừ đóng hộp thường rẻ hơn cá tươi. Hầu như cửa hàng thực phẩm nào cũng có các mặt hàng giảm giá hàng tuần mà bạn có thể thấy trên các tờ quảng cáo của họ. Khi một món hàng giảm giá thi đó chính là dịp bạn nên tích trữ. Ví dụ, hầu hết các thực phẩm protein đều có thể trữ đông và để dành được, vì vậy nếu cửa hàng giảm giá ức gà rút xương bỏ da, bạn hãy mua nhiều hơn một chút để ăn ngay và cất trữ để dùng sau. Có lẽ bạn đã biết tìm giá rẻ nhất khi mua hàng, nhưng đôi khi bạn vẫn giữ thói quen lần nào cũng mua một nhãn hàng và cứ đinh ninh là rẻ nhất vì trước giờ vẫn vậy. Hãy cẩn thận, vì giá cả luôn luôn thay đổi. Thỉnh thoảng người ta giảm giá cho một số gói hàng có kích cỡ nhất định hoặc một lô hàng nào đó, và do vậy lựa chọn có lợi nhất cũng thay đổi.  Hãy nhìn khắp lượt từ trên xuống dưới, vì các cửa hàng thực phẩm thường bày những mặt hàng đắt nhất ở ngang tầm mắt. Để ý đến các sản phẩm mang nhãn hiệu của cửa hàng – chúng thường rẻ hơn các sản phẩm của các nhãn hiệu khác. Mặc dù bạn sẽ phải trả nhiều tiền hơn khi mua hàng với số ượng lớn, nhưng nhìn tổng thể thì chúng thường rẻ hơn. Chẳng hạn như một lon bột yến mạch ăn liền có giá rẻ hơn nhiều so với hộp yến mạch gồm nhiều gói nhỏ. Bạn có thể thêm hoa quả tươi để tạo ra các hương vị riêng.  Cân nhắc mua một số sản phẩm đựng trong hộp lớn ở các cửa hàng thực phẩm bảo vệ sức khỏe. Các cửa hàng này thường bán các mặt hàng như ngũ cốc, các loại đậu, mì, quả hạch, bánh granolas, bột, đường. Bạn có thể chỉ cần mua đủ dùng mà vẫn tận dụng được mức giá rẻ hơn. Chú ý canh thời gian sử dụng sản phẩm khi mua số lượng lớn. Đừng để bị rơi vào bẫy mua cả một hộp mayonnaise thật to mà bạn không thể nào dùng hết trước khi hết hạn, hoặc một hộp ngũ cốc lớn mà cả nhà chẳng ai thích ăn. Hãy tận dụng các thẻ giảm giá nếu bạn tình cờ nhận được. Tuy nhiên, bạn chỉ nên dùng thẻ giảm giá mua những món hàng mà đằng nào cũng phải mua. Dù rẻ hơn, nhưng có thể bạn sẽ tốn thêm tiền cho một thứ không ưng ý khi mua một thứ mà bình thường bạn không mua. Bạn có thể tìm các thẻ giảm giá trên mạng và trên báo, hoặc dùng nhiều ứng dụng thẻ giảm giá để tiết kiệm tiền. Bạn có thể đủ điều kiện để được trợ cấp theo các chương trình này nếu có thu nhập thấp. SNAP là tên gọi mới của chương trình tem phiếu thực phẩm, theo đó bạn sẽ được hỗ trợ qua thẻ dạng ghi nợ. WIC chỉ dành cho phụ nữ thu nhập thấp và có con nhỏ. Phụ nữ mang thai cũng đủ điều kiện được trợ cấp. Tuy có giới hạn về những mặt hàng được mua, nhưng chương trình này cũng vẫn giúp bạn mua được các thực phẩm lành mạnh.  Bạn có thể đến văn phòng SNAP ở địa phương để đăng ký. Nhiều bang cũng cho phép bạn nộp đơn trên mạng. Hãy vào trang https://www.fns.usda.gov/snap/apply xem bạn có thể đăng ký trên mạng không. Để nộp đơn xin trợ cấp theo chương trình WIC, bạn hãy liên hệ với văn phòng WIC ở địa phương. Đôi khi  bạn có thể mua được các sản phẩm có giá phải chăng ở các chợ của nông dân, mặc dù phải chịu khó tìm kiếm. Ngoài ra, bạn cũng có thể đến các nông trại trong vùng để chọn cho mình các sản phẩm rẻ hơn. Bạn cần sử dụng sớm những thứ mua được, vì các sản phẩm tươi ở nông trại không để được lâu như các sản phẩm ở cửa hàng vốn được lai giống và xử lý để tăng thời hạn sử dụng. Nhưng bù lại, bạn sẽ được thưởng thức hương vị tuyệt vời của chúng. Một số chợ của nông dân thậm chí còn chấp nhận thẻ SNAP.',\n",
       " 'Không phải tất cả các nguyên liệu lành mạnh đều đắt đỏ. Thực ra, nhiều nguyên liệu lành mạnh lại khá rẻ. Bạn hãy xem xét từng nhóm thực phẩm cần mua và lựa chọn các sản phẩm của từng nhóm.  Ví dụ, trong nhóm ngũ cốc, bạn có thể thử mua yến mạch, gạo lứt, tấm lúa mì, bỏng ngô, mì và bánh mì làm từ lúa mì nguyên hạt. Với rau củ, bạn hãy chọn những loại rau như bắp cải, rau lá xanh (như cải xanh, cải xoăn, thậm chí bông cải xanh), bí xanh, cà rốt và cần tây. Với hoa quả, bạn nên mua các loại quả rẻ hơn như cam, táo và chuối. Về các sản phẩm sữa, bạn hãy chọn sữa và sữa chua trắng đựng trong các lọ lớn. Bạn có thể tự thêm hương vị cho sữa và sữa chua mà không tốn kém là bao. Hãy nhớ rằng thường thì chúng ta cần ít protein hơn chúng ta tưởng. Để dành protein ăn trong nhiều bữa cũng là một cách tiết kiệm tiền mà lại tốt cho chế độ dinh dưỡng. Ví dụ,  một hôm nào đó làm món gà ăn tối, hôm sau bạn hãy dùng phần gà còn thừa để nấu món súp gà. Ngày hôm sau nữa, bạn có thể dùng gà làm nhân bánh kẹp.  Phụ nữ dưới 30 tuổi mỗi ngày cần khẩu phần tính theo ounce là 5 ½ (155 g), trong khi phụ nữ trên 30 tuổi chỉ cần 5 ounce (140 g) . Nam giới dưới 30 tuổi cần 6 ½ ounce (185 g), nhưng nam giới trong độ tuổi 30-50 chỉ cần ounce (170 g) và nam giới trên 50 tuổi chỉ cần 5 ½ ounce (155 g). Một khẩu phần tương đương 1 ounce (28 g) thịt. (3 ounce thịt có kích cỡ tương đương một bộ bài). Mặc dù bạn không cần phải cắt hết protein động vật khỏi chế độ ăn, nhưng việc chọn các loại protein thực vật đôi khi cũng có lợi cho túi tiền của bạn. Thỉnh thoảng bạn cũng nên chọn các bữa ăn chay, chẳng hạn như một bữa tối ăn cơm với đậu thay vì ăn thịt hầm.  Một số khẩu phần chay tương đương với 1 ounce thịt bao gồm: 1 quả trứng, 1/4 cốc (60 ml) đậu hoặc đậu lăng, 15 g quả hạch hoặc các loại hạt, 1 thìa canh (15 ml) bơ lạc, hoặc 2 thìa canh (30 ml)  hummus (món ăn làm từ đậu và các gia vị).  Tăng lượng protein qua rau củ và carbohydrate phức. Khi kết hợp một lượng nhỏ thịt với các nguyên liệu lành mạnh khác, bạn sẽ có bữa ăn không chỉ tốt cho sức khỏe mà còn no lâu hơn. Hãy cân nhắc:  Món xào Bánh taco Các món mì Khi so sánh giá cả của thực phẩm, bạn cũng nên so sánh các thành phần ghi trên nhãn, đặc biệt nếu bạn mua các loại thực phẩm đóng gói sẵn. Ví dụ, nếu phải mua một hộp mì ống sốt phô mai, tốt nhất là bạn nên chọn loại lành mạnh nhất có thể.  Tìm các thực phẩm ít đường và muối. Bạn nên ăn 2.300 mg (1 thìa cà phê) muối mỗi ngày. Ngoài ra, bạn cũng nên tìm loại có ít chất béo chuyển hóa và chất béo bão hòa. Ngay cả các chất béo lành mạnh hơn cũng chỉ nên chiếm 20-30% lượng thức ăn. Duy trì lượng calo ở mức 400 hoặc thấp hơn. Bên cạnh đó, bạn cũng nên kiểm tra về lượng vitamin và khoáng chất trong sản phẩm định mua.',\n",
       " 'Thường thì bạn có thể tận dụng một khoảnh vườn nhỏ, thậm chí vài chậu cây để trồng rau ăn mà không mấy tốn kém. Thử trồng cà chua hoặc vài loại rau thơm trên bệ cửa sổ chẳng hạn. Bạn có thể đến cửa hàng mua các loại bánh ăn vặt lành mạnh hơn một chút so với các loại thông thường, chẳng hạn như rau củ chiên hoặc hoa quả trộn. Tuy nhiên, các loại này thường khá đắt, vì vậy sẽ tiết kiệm hơn nếu bạn tự làm lấy.  Ví dụ, bạn hãy thử làm món cải xoăn giòn. Rửa cải xoăn và để cho ráo hết nước, sau đó cắt hoặc xé các lá cải thành từng mảnh lớn. Nhúng rau vào dầu ô liu hoặc xịt dầu ăn chống dính và trải thành một lớp trên khay nướng lớn. Rắc muối tiêu hoặc bất cứ gia vị nào bạn thích lên trên và nướng ở nhiệt độ 177 độ C cho đến khi giòn (khoảng 15 phút). Bạn cũng có thể tự làm những cốc rau củ hoặc hoa quả trộn. Cắt nhỏ cam, táo, bưởi và thêm vào một chút mật ong. Chia thành các hộp nhỏ dùng được nhiều lần để bạn có thể ăn bất cứ lúc nào. Bạn cũng có thể chế biến tương tự với rau. Cắt rau thành từng miếng vừa ăn và bỏ vào trong các túi sử dụng nhiều lần. Đựng món hummus (tự làm) vào từng hộp để có thể đem đi. Nước dùng là nguyên liệu tuyệt vời để nấu súp, nhưng các hộp nước dùng bán ở cửa hàng có thể khá đắt. Hơn nữa các sản phẩm này thường có nhiều muối. Nước dùng nấu tại nhà có chất lượng hơn và còn rẻ hơn.  Thậm chí bạn có thể tận dụng những mảnh vụn để nấu nước dùng. Giữ lại các mẩu rau củ đầu thừa đuôi thẹo như vỏ hành tây, đuôi cà rốt hoặc đầu cần tây. Khi ăn gà, bạn có thể giữ lại xương và các mẩu thịt thừa, bỏ vào túi và đông lạnh cho đến khi đủ nấu. Khi nấu, bạn chỉ cần bỏ hết vào nồi, đổ nước vào và đun sôi (đậy vung và vặn nhỏ lửa) trong 6-8 tiếng hoặc cho đến khi đạt được hương vị mong muốn. Lọc lấy nước, vậy là bạn đã có nước dùng để sử dụng. Khi đã nấu xong nước dùng, bạn có thể chia thành từng phần nhỏ và bảo quản bằng cách đông lạnh. Ngoài đồ ăn vặt và nước dùng, bạn có thể tự làm nhiều thực phẩm khác với chi phí rẻ hơn. Ví dụ, nếu bạn thường ăn sữa chua thì làm sữa chua sẽ là lựa chọn rất hay. Bạn cũng có thể cân nhắc mua máy làm bánh mì, và việc làm bánh mì sẽ trở nên đơn giản hơn nhiều.',\n",
       " \"Việc lập thực đơn sẽ giúp bạn chỉ mua những thứ cần thiết, và như vậy sẽ không vượt quá ngân sách. Nếu chưa có các công thức nấu ăn yêu thích mà lành mạnh, bạn hãy tìm trên mạng, trong các sách dạy nấu ăn hoặc nhờ bạn bè giới thiệu.  Thử vào trang What's Cooking của USDA (Bộ nông nghiệp Hoa Kỳ) (https://whatscooking.fns.usda.gov/), trong đó có các công thức nấu những món ăn tốt cho sức khỏe mà bạn có thể dùng để lập thực đơn. Đừng quên lập thực đơn dựa vào thời gian biểu của bạn. Bạn có thể sử dụng các thức ăn còn thừa hoặc các bữa ăn nhanh vào những hôm bận nhiều việc. Có thể bạn cho rằng ăn uống lành mạnh là cần phải thử nhiều loại thực phẩm theo trào lưu mới mà mọi người đồn rằng tốt cho sức khỏe. Các thực phẩm này có thể lành mạnh thật, nhưng không có nghĩa chúng là thực phẩm duy nhất tốt cho sức khỏe. Hãy dựa vào các thức ăn đơn giản mà bạn đã biết, thậm chí chỉ là món gà nướng ăn với rau và cơm gạo lứt. Như vậy bạn vẫn có thể tiếp tục ăn uống lành mạnh mà lại còn tiết kiệm được tiền. Sau khi đã lập thực đơn, bạn hãy liệt kê những thứ cần mua. Nhớ bám sát vào danh sách (và đừng mua nhiều), như vậy bạn sẽ không chi tiêu vượt mức và có đủ tiền để mua các thức ăn lành mạnh hơn. Kế hoạch ăn uống không chỉ để áp dụng ở nhà mà nó còn giúp bạn dự tính trước khi đến nhà hàng. Hãy tìm xem thực đơn của họ trên mạng nếu có và so sánh lượng calo. Nhiều nhà hàng có ghi lượng calo trong thực đơn, nhưng bạn có thể tính toán trên bảng tính calo online  hoặc các trang web.  Cân nhắc chọn món khai vị lành mạnh để tiết kiệm tiền. Món khai vị cũng sẽ giúp giảm khẩu phần ăn. Chọn món chính gồn protein nạc ăn kèm với ngũ cốc nguyên hạt và nhiều rau. Chọn rau hoặc hoa quả thay vì các món chiên, hành tây rán hoặc khoai tây nghiền. Xin hộp đựng thức ăn đem về khi gọi món. Trước khi ăn, bạn hãy chia đôi phần ăn và bỏ một nửa vào hộp để đem về nhà; như vậy bạn sẽ kiểm soát được khẩu phần ăn của mình mà còn tiết kiệm nữa.\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"]['article'][0]['document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0fdb1f5c-e98c-444b-913f-17eb76e94b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [d.get(\"document\")[0] for d in dataset[\"train\"]['article']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5de253ca-8a91-4b06-aaeb-3ad732956c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c50e3b2f-0d8d-4b0d-b0b7-405e3f40be03",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 83001e09-6417-4d6a-9509-e8791b60ebbe)')' thrown while requesting HEAD https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "119547\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mis_fast)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mvocab_size)\n\u001b[0;32m----> 5\u001b[0m train_data \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama-adapter/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2365\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2328\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2329\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2330\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2350\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2351\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2352\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2363\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[1;32m   2364\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2365\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2366\u001b[0m         text,\n\u001b[1;32m   2367\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   2368\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2369\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2370\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   2371\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2372\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2373\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2374\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2375\u001b[0m     )\n\u001b[1;32m   2377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/llama-adapter/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2773\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2763\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2764\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2765\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2766\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2770\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2771\u001b[0m )\n\u001b[0;32m-> 2773\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[1;32m   2774\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2775\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   2776\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2777\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m   2778\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   2779\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2780\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2781\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2782\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2783\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2784\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   2785\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   2786\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   2787\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   2788\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   2789\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   2790\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   2791\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2792\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama-adapter/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:517\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    497\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    515\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    516\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 517\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[1;32m    518\u001b[0m         batched_input,\n\u001b[1;32m    519\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    520\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    521\u001b[0m         padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    522\u001b[0m         truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m    523\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m    524\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m    525\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    526\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m    527\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m    528\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    529\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m    530\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m    531\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m    532\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m    533\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    535\u001b[0m     )\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama-adapter/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:445\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    438\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    439\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    442\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    443\u001b[0m )\n\u001b[0;32m--> 445\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_batch(\n\u001b[1;32m    446\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    447\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    448\u001b[0m     is_pretokenized\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    449\u001b[0m )\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    457\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    459\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    469\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "print(tokenizer.is_fast)\n",
    "print(tokenizer.vocab_size)\n",
    "train_data = tokenizer.encode(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "22cfd2f4-459a-4739-a1b3-50770113e118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d7d7a500-e353-45f6-89c0-effae3385803",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\n",
    "        data,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "18063d10-a97f-4e5b-b3ac-d6010bec067b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef78f0a5-1ab6-4b0d-b8cc-c2938adcb5c1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 164,\n",
       " 112,\n",
       " 15475,\n",
       " 25308,\n",
       " 10432,\n",
       " 11859,\n",
       " 10138,\n",
       " 16591,\n",
       " 186,\n",
       " 29173,\n",
       " 14789,\n",
       " 12072,\n",
       " 11603,\n",
       " 27637,\n",
       " 119,\n",
       " 51635,\n",
       " 38259,\n",
       " 18004,\n",
       " 10792,\n",
       " 43872,\n",
       " 18031,\n",
       " 11992,\n",
       " 24274,\n",
       " 16807,\n",
       " 39140,\n",
       " 24287,\n",
       " 11257,\n",
       " 10792,\n",
       " 20047,\n",
       " 18031,\n",
       " 47575,\n",
       " 16913,\n",
       " 21080,\n",
       " 11603,\n",
       " 27637,\n",
       " 17819,\n",
       " 10876,\n",
       " 21820,\n",
       " 38470,\n",
       " 18031,\n",
       " 53575,\n",
       " 18643,\n",
       " 39930,\n",
       " 299,\n",
       " 53725,\n",
       " 11934,\n",
       " 17031,\n",
       " 27258,\n",
       " 11603,\n",
       " 13199,\n",
       " 20264,\n",
       " 27258,\n",
       " 10504,\n",
       " 10558,\n",
       " 119,\n",
       " 145,\n",
       " 66316,\n",
       " 42259,\n",
       " 11859,\n",
       " 10138,\n",
       " 25308,\n",
       " 82540,\n",
       " 27637,\n",
       " 18323,\n",
       " 29656,\n",
       " 13284,\n",
       " 10743,\n",
       " 10263,\n",
       " 14789,\n",
       " 119,\n",
       " 29561,\n",
       " 27637,\n",
       " 23886,\n",
       " 117,\n",
       " 43094,\n",
       " 19114,\n",
       " 33090,\n",
       " 10792,\n",
       " 16913,\n",
       " 24274,\n",
       " 12552,\n",
       " 57696,\n",
       " 10743,\n",
       " 16218,\n",
       " 117,\n",
       " 57696,\n",
       " 27637,\n",
       " 23164,\n",
       " 117,\n",
       " 45105,\n",
       " 170,\n",
       " 46916,\n",
       " 10432,\n",
       " 30185,\n",
       " 10133,\n",
       " 119,\n",
       " 110433,\n",
       " 23164,\n",
       " 12002]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2cb2b304-9f10-4322-8f65-45399e83b942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Hoa quả và rau thường rẻ hơn khi vào mùa. Thường thì các cửa hàng thực phẩm sẽ giảm giá cho các mặt hàng nông sản đang vào mùa vì họ biết khách hàng luôn chờ đợi những loại nào vào thời gian nào trong năm. Hơn nữa rau quả đúng mùa bao giờ cũng ngon hơn. Vào mùa thu, bạn nên chọn các sản phẩm như bí ngô, bí mùa đông, cải bắp và táo. Mùa đông đến, bạn hãy tìm bí mùa đông, cải bắp và các loại củ khác. Mùa xuân là mùa của các loại rau củ như củ cải, rau lá xanh, hành lá và măng tây. Mùa hè là dịp tốt để thưởng thức nhiều loại rau quả, đơn cử như dưa hấu, ngô và các loại quả mọng. Có lẽ bạn cũng nhận thấy là giá cả của rau củ quả thường rẻ nhất trong mùa hè. Ngoài ra, vì nông sản mùa hè rẻ hơn nên bạn có thể mua nhiều để đông lạnh hoặc tự đóng hộp. Có lẽ bạn thích hoa quả và rau củ tươi hơn, nhưng thường thì bạn có thể tiết kiệm hơn nếu chọn sản phẩm đông lạnh hoặc đóng hộp. Những sản phẩm này cũng có lợi cho sức khỏe tương đương như sản phẩm tươi, nhưng bạn cần chọn các loại không thêm đường hoặc muối. Bạn cũng nên xem xét cả nhóm thực phẩm protein. Gà đông lạnh có thể rẻ hơn gà tươi ; cá hồi và cá ngừ đóng hộp thường rẻ hơn cá tươi. Hầu như cửa hàng thực phẩm nào cũng có các mặt hàng giảm giá hàng tuần mà bạn có thể thấy trên các tờ quảng cáo của họ. Khi một món hàng giảm giá thi đó chính là dịp bạn nên tích trữ. Ví dụ, hầu hết các thực phẩm protein đều có thể trữ đông và để dành được, vì vậy nếu cửa hàng giảm giá ức gà rút xương bỏ da, bạn hãy mua nhiều hơn một chút để ăn ngay và cất trữ để dùng sau. Có lẽ bạn đã biết tìm giá rẻ nhất khi mua hàng, nhưng đôi khi bạn vẫn giữ thói quen lần nào cũng mua một nhãn hàng và cứ đinh ninh là rẻ nhất vì trước giờ vẫn vậy. Hãy cẩn thận, vì giá cả luôn [SEP]'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "068798f1-8f6b-4ae7-8845-227cebcbab6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "32000\n"
     ]
    }
   ],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "print(tokenizer2.is_fast)\n",
    "print(tokenizer2.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "efcb156b-3107-4803-bdad-223a804b66ae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "int too big to convert",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer2\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39munk_token\n\u001b[0;32m----> 2\u001b[0m input_ids2 \u001b[38;5;241m=\u001b[39m tokenizer2(\n\u001b[1;32m      3\u001b[0m         data,\n\u001b[1;32m      4\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mtokenizer2\u001b[38;5;241m.\u001b[39mmodel_max_length,\n\u001b[1;32m      7\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     )\u001b[38;5;241m.\u001b[39minput_ids\n",
      "File \u001b[0;32m~/miniconda3/envs/llama-adapter/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2594\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2592\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2593\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2594\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[1;32m   2595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2596\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda3/envs/llama-adapter/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2680\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2675\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2676\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2677\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2678\u001b[0m         )\n\u001b[1;32m   2679\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2680\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2681\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2682\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2683\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2684\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   2685\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2686\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2687\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2688\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2689\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2690\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   2691\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   2692\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   2693\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   2694\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   2695\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   2696\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   2697\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2698\u001b[0m     )\n\u001b[1;32m   2699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2701\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2702\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2718\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2719\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama-adapter/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2871\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2862\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2863\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2864\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2868\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2869\u001b[0m )\n\u001b[0;32m-> 2871\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[1;32m   2872\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2873\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2874\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m   2875\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   2876\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2877\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2878\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2879\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2880\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2881\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   2882\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   2883\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   2884\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   2885\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   2886\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   2887\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   2888\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2889\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama-adapter/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:437\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    433\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_text_or_text_pairs has to be a list or a tuple (got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(batch_text_or_text_pairs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    434\u001b[0m     )\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    438\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    439\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m    440\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m    441\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m    442\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    443\u001b[0m )\n\u001b[1;32m    445\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_batch(\n\u001b[1;32m    446\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    447\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    448\u001b[0m     is_pretokenized\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    449\u001b[0m )\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama-adapter/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:392\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.set_truncation_and_padding\u001b[0;34m(self, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of)\u001b[0m\n\u001b[1;32m    389\u001b[0m         current \u001b[38;5;241m=\u001b[39m {k: _truncation\u001b[38;5;241m.\u001b[39mget(k, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m target}\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m current \u001b[38;5;241m!=\u001b[39m target:\n\u001b[0;32m--> 392\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39menable_truncation(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtarget)\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_strategy \u001b[38;5;241m==\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _padding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOverflowError\u001b[0m: int too big to convert"
     ]
    }
   ],
   "source": [
    "tokenizer2.pad_token = tokenizer.unk_token\n",
    "input_ids2 = tokenizer2(\n",
    "        data,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer2.model_max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d6ab5ed2-8650-4ac2-a79e-341f764511b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Hoa quả và rau thường rẻ hơn khi vào mùa. Thường thì các cửa hàng thực phẩm sẽ giảm giá cho các mặt hàng nông sản đang vào mùa vì họ biết khách hàng luôn chờ đợi những loại nào vào thời gian nào trong năm. Hơn nữa rau quả đúng mùa bao giờ cũng ngon hơn.  Vào mùa thu, bạn nên chọn các sản phẩm như bí ngô, bí mùa đông, cải bắp và táo. Mùa đông đến, bạn hãy tìm bí mùa đông, cải bắp và các loại củ khác. Mùa xuân là mùa của các loại rau củ như củ cải, rau lá xanh, hành lá và măng tây. Mùa hè là dịp tốt để thưởng thức nhiều loại rau quả, đơn cử như dưa hấu, ngô và các loại quả mọng. Có lẽ bạn cũng nhận thấy là giá cả của rau củ quả thường rẻ nhất trong mùa hè. Ngoài ra, vì nông sản mùa hè rẻ hơn nên bạn có thể mua nhiều để đông lạnh hoặc tự đóng hộp'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.decode(input_ids2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "181fa581-22cb-4961-9f91-07cb410f57a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6616"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cfc3c6f9-44f9-4bcf-bd3e-6b428b980577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000000000000019884624838656"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7443865b-788a-44ae-9a3a-2c8e46588f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPLIT IS 5954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4396,  641, 4601, 3046, 1132,  451, 1523, 3043, 4995, 5189])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split train/test\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "perm = np.random.permutation(len(input_ids2))\n",
    "split = int(len(perm) * 0.90)\n",
    "print(f\"SPLIT IS {split}\")\n",
    "train_indices = perm[:split]\n",
    "eval_indices = perm[split:]\n",
    "train_indices[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "440fad0d-5317-46b2-8c04-cb466c51d978",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [input_ids2[i] for i in train_indices]\n",
    "eval_data = [input_ids2[i] for i in eval_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b1f92ed7-6392-4e78-a680-81039a1d3ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Bạn có thể áp dụng phương pháp này trên cả Internet Explorer và Firefox.  Nếu dùng Microsoft Edge, bạn cần mở Internet Explorer để tiến hành, vì trình duyệt Edge không hỗ trợ tính năng này. Bất kể trình duyệt mặc định của bạn là gì thì khi nhấp đúp vào shortcut, trang web thường sẽ được mở bằng trình duyệt mà bạn đã dùng để tạo lối tắt. Mở chính xác trang mà bạn muốn tạo shortcut. Bạn có thể tạo lối tắt cho bất kỳ trang web nào nhưng vẫn cần phải đăng nhập nếu website luôn yêu cầu. Bạn cần thấy được màn hình nền để thao tác dễ dàng hơn. Bạn sẽ thấy biểu tượng trang web hơi mờ di chuyển cùng con trỏ chuột. Shortcut cùng tên với tiêu đề trang web  sẽ xuất hiện trên màn hình. Lối tắt có thể mang biểu tượng của website (nếu có). Nếu bạn dùng Internet Explorer để tạo shortcut thì trang web này sẽ luôn được mở trong Internet Explorer. Nhưng nếu bạn tạo bằng Firefox thì website sẽ '"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.decode(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b6d78b4e-d497-47a1-81d6-c668d62b9c4b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,   350, 30540, 29876, 28810,   266, 31957,  3976, 29886,   270,\n",
       "        31620,   865,  1374, 30416, 30556,   865,  1374, 29976, 29886,   302,\n",
       "        30001, 29891,   534,  5512,   274, 30643,  4685, 21508,   325, 30001,\n",
       "        14418, 29889, 29871,   405, 30717, 29884,   270, 30071,   865,  7783,\n",
       "        21086, 29892,   289, 30540, 29876,   274, 30884, 29876,   286,   228,\n",
       "          190,   162,  4685, 21508, 29871, 30128, 31957, 19538, 30717, 29876,\n",
       "          298, 24267, 29882, 29892,   325, 30097,   534, 30097, 29876, 29882,\n",
       "          868, 29891, 30529, 29873, 21086,   413, 29882, 30069,   865,   298,\n",
       "          228,   190,   154,   534, 31645,   260,  3642, 29882,   302, 30035,\n",
       "          865,   302, 30001, 29891, 29889,   350, 31145, 29873,   413, 31957,\n",
       "          534, 30097, 29876, 29882,   868, 29891, 30529, 29873,   286,   228,\n",
       "          189,   186, 29883, 29871, 30128, 30740, 29876, 29882,   274, 31556,\n",
       "        29874,   289, 30540, 29876, 18916,   330, 30097,   266, 30097,   413,\n",
       "         2918,   302, 29882, 31145, 29886, 29871, 30128, 30030, 29886,   325,\n",
       "        30001, 29877, 21697, 29892,   534,   574,  1856,   266, 30416, 30997,\n",
       "          865,   269,   228,   189,   192, 29871, 30128, 30416, 31645, 29883,\n",
       "          286,   228,   190,   162,   289,   228,   189,   180,   865,   534,\n",
       "        30097, 29876, 29882,   868, 29891, 30529, 29873,   286, 30001,   289,\n",
       "        30540, 29876, 29871, 30128, 30033,   270, 30071,   865, 29871, 30128,\n",
       "        31957,   260, 30540, 29877,   301, 30984, 29875,   260, 31160, 29873,\n",
       "        29889,   341,   228,   190,   162,   521,  3642, 29882,   921, 25968,\n",
       "          534,   574,   286, 30001,   289, 30540, 29876,  3887, 30984, 29876,\n",
       "          260, 30540, 29877, 21697, 29889,   350, 30540, 29876, 28810,   266,\n",
       "        31957,   260, 30540, 29877,   301, 30984, 29875,   260, 31160, 29873,\n",
       "         3060,   289, 31145, 29873,   413, 31842,   534,   574,  1856,   302,\n",
       "        30001, 29877,   302, 29882, 30416,   865,   325,   228,   189,   174,\n",
       "        29876,   274, 30884, 29876,  1374, 30643, 29875, 29871, 30128, 30035,\n",
       "          865,   302, 29882, 31142, 29886,   302, 30717, 29884,  4700,  8092,\n",
       "        30069, 29876,   343, 30037, 29884,   274, 30884, 29884, 29889,   350,\n",
       "        30540, 29876,   274, 30884, 29876,   266, 31145, 29891, 29871, 30128,\n",
       "        30416, 31645, 29883,   286, 24267,   298, 30097, 29876, 29882,   302,\n",
       "        31343, 29876, 29871, 30128, 31957,   266,  6241,   260, 25968,   270,\n",
       "        30701,   270, 30001,   865,   298, 30556, 29876, 29889,   350, 30540,\n",
       "        29876,   269,   228,   189,   192,   266, 31145, 29891,  4768, 31957,\n",
       "        29884,   260, 30416, 31645,   865,   534,   574,  1856,   298, 30556,\n",
       "        29875,   286, 30997,   652,   521,  8631, 31957, 29876,   274, 30071,\n",
       "          865,   378,   534,   228,   190,   146,   521, 29884, 30902, 29873,\n",
       "        29889, 13899,  7582,   274, 30071,   865,   260,  5512,   325, 31280,\n",
       "        29875, 19538, 30037, 29884, 29871, 30128, 31343,   534,   574,  1856,\n",
       "        29871,   269,   228,   189,   192,   921, 29884, 31145, 29873,  7251,\n",
       "        30529, 29876,   534,  5512,   286, 24267,   298, 30097, 29876, 29882,\n",
       "        29889,   365, 30984, 29875,   260, 31160, 29873, 28810,   266, 31957,\n",
       "        25016,  4768, 31957, 29884,   260, 30416, 31645,   865,   274, 31556,\n",
       "        29874,  4700,   313, 29876, 30717, 29884, 28810,   467,   405, 30717,\n",
       "        29884,   289, 30540, 29876,   270, 30071,   865,  4685, 21508, 29871,\n",
       "        30128, 31957,   260, 30540, 29877, 21697,   266, 30097,   534,   574,\n",
       "         1856,   302, 30001, 29891,   269,   228,   189,   192,  8092, 30069,\n",
       "        29876, 29871, 30128, 30416, 31645, 29883,   286,   228,   190,   162,\n",
       "          534,   549,  4685, 21508, 29889,   405, 29882, 30416,   865,   302,\n",
       "        30717, 29884,   289, 30540, 29876,   260, 30540, 29877,   289,   228,\n",
       "          189,   180,   865, 14418,   266, 30097,  4700,   269,   228,   189,\n",
       "          192, 29871])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "64ca3f91-752f-4a19-b78f-0f7d597e4662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bạn'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.decode([350, 30540, 29876])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bc5ed3c5-5416-4f23-95ee-1a20783c18bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e5ecc23c-326f-43de-97b4-172519c927d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = torch.randint(high=len(train_data) - 512, size=(16,))\n",
    "x = torch.stack([train_data[i] for i in ix])  # Stack training examples in a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "13c50996-fe01-425d-b599-786f7f83e36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "741a3754-9e85-4d16-97fe-49f3c852b223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4505,  201, 1845, 2473,  658, 3264, 5126, 3473,  423, 4524, 3429, 2978,\n",
       "        2391, 5340, 1932, 2379])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed74a363-be0a-46ef-bd3d-956cc2b39b27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-adapter",
   "language": "python",
   "name": "llama-adapter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
